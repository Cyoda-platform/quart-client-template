{
    "entity_models": [
        {
            "entity_model_name": "entity_job",
            "workflow_function": {
                "code_with_necessary_imports_and_constants": "import asyncio\nimport logging\nfrom datetime import datetime",
                "content": "async def process_entity_job(entity: dict) -> dict:\n    \"\"\"\n    Workflow function for entity_job.\n    Runs async processing before entity persistence.\n    Modifies entity in place to update status, results, or error.\n    \"\"\"\n    try:\n        job_id = entity.get('job_id', 'unknown')\n        csv_url = entity.get('csv_url')\n        subscribers = entity.get('subscribers', [])\n        analysis_type = entity.get('analysis_type', 'summary')\n\n        if not csv_url:\n            raise ValueError(\"csv_url is missing\")\n\n        logger.info(f\"Job {job_id}: Starting data download from {csv_url}\")\n        df = await download_csv(csv_url)\n\n        logger.info(f\"Job {job_id}: Data downloaded, starting analysis\")\n        analysis_result = analyze_data(df, analysis_type)\n\n        logger.info(f\"Job {job_id}: Analysis complete, sending emails\")\n        await send_email_report(subscribers, analysis_result)\n\n        # Update entity state before persistence\n        entity[\"status\"] = \"completed\"\n        entity[\"result\"] = analysis_result\n        entity[\"last_processed_at\"] = datetime.utcnow().isoformat() + \"Z\"\n\n        logger.info(f\"Job {job_id}: Completed successfully\")\n\n    except Exception as e:\n        job_id = entity.get('job_id', 'unknown')\n        logger.exception(f\"Job {job_id}: Failed with exception\")\n        entity[\"status\"] = \"failed\"\n        entity[\"error\"] = str(e)\n\n    return entity",
                "name": "process_entity_job"
            }
        }
    ],
    "file_without_workflow": {
        "code": "from dataclasses import dataclass\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nimport httpx\nimport pandas as pd\nfrom quart import Quart, jsonify, request\nfrom quart_schema import QuartSchema, validate_request\n\nimport json\nfrom app_init.app_init import BeanFactory\nfrom common.config.config import ENTITY_VERSION\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\napp = Quart(__name__)\nQuartSchema(app)\n\nfactory = BeanFactory(config={'CHAT_REPOSITORY': 'cyoda'})\nentity_service = factory.get_services()['entity_service']\ncyoda_auth_service = factory.get_services()[\"cyoda_auth_service\"]\n\n@dataclass\nclass ProcessDataRequest:\n    csv_url: str\n    subscribers: list[str]\n    analysis_type: str\n\nasync def download_csv(url: str) -> pd.DataFrame:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        response.raise_for_status()\n        # Use StringIO to read CSV from string content\n        from io import StringIO\n        df = pd.read_csv(StringIO(response.text))\n        return df\n\ndef analyze_data(df: pd.DataFrame, analysis_type: str) -> Dict[str, Any]:\n    # Defensive: if df empty, return empty summary\n    if df.empty:\n        return {\n            \"rows_processed\": 0,\n            \"columns\": [],\n            \"statistics\": {},\n            \"note\": \"Empty dataset\",\n        }\n    if analysis_type == \"summary\":\n        desc = df.describe(include='all').to_dict()\n        result = {\n            \"rows_processed\": len(df),\n            \"columns\": df.columns.tolist(),\n            \"statistics\": desc,\n        }\n    else:\n        desc = df.describe(include='all').to_dict()\n        result = {\n            \"rows_processed\": len(df),\n            \"columns\": df.columns.tolist(),\n            \"statistics\": desc,\n            \"note\": f\"Analysis type '{analysis_type}' not implemented, returning summary.\",\n        }\n    return result\n\nasync def send_email_report(subscribers: list[str], report: Dict[str, Any]) -> None:\n    # TODO: Replace with real email sending logic (SMTP / email API)\n    if not subscribers:\n        logger.warning(\"No subscribers specified for email report\")\n        return\n    try:\n        logger.info(f\"Sending report email to subscribers: {subscribers}\")\n        logger.info(f\"Report summary: Rows processed: {report.get('rows_processed')}, Columns: {report.get('columns')}\")\n        await asyncio.sleep(1)  # simulate sending email delay\n    except Exception as e:\n        logger.error(f\"Failed to send email report: {e}\")\n\n@app.route(\"/api/process-data\", methods=[\"POST\"])\n@validate_request(ProcessDataRequest)\nasync def process_data(data: ProcessDataRequest):\n    job_id = datetime.utcnow().strftime(\"%Y%m%d%H%M%S%f\")\n\n    # Initial entity with status processing and input data\n    entity = {\n        \"job_id\": job_id,\n        \"status\": \"processing\",\n        \"requestedAt\": datetime.utcnow().isoformat() + \"Z\",\n        \"csv_url\": data.csv_url,\n        \"subscribers\": data.subscribers,\n        \"analysis_type\": data.analysis_type,\n    }\n\n    try:\n        # Pass workflow function which will run all async logic before persistence\n        technical_id = await entity_service.add_item(\n            token=cyoda_auth_service,\n            entity_model=\"entity_job\",\n            entity_version=ENTITY_VERSION,\n            entity=entity\n        )\n    except Exception as e:\n        logger.exception(\"Failed to add entity_job to entity_service\")\n        return jsonify({\"status\": \"error\", \"message\": \"Could not initiate processing\"}), 500\n\n    return jsonify({\n        \"status\": \"processing_started\",\n        \"message\": \"Data processing and email sending initiated.\",\n        \"job_id\": job_id,\n    }), 202\n\n@app.route(\"/api/results\", methods=[\"GET\"])\nasync def get_results():\n    try:\n        items = await entity_service.get_items(\n            token=cyoda_auth_service,\n            entity_model=\"entity_job\",\n            entity_version=ENTITY_VERSION,\n        )\n    except Exception as e:\n        logger.exception(\"Failed to get items from entity_service\")\n        return jsonify({\"status\": \"error\", \"message\": \"Failed to get results\"}), 500\n\n    if not items:\n        return jsonify({\"status\": \"no_jobs\", \"message\": \"No processing jobs found.\"})\n\n    latest_job = None\n    latest_requested_at = \"\"\n    for item in items:\n        req_at = item.get(\"requestedAt\", \"\")\n        if req_at > latest_requested_at:\n            latest_requested_at = req_at\n            latest_job = item\n\n    if not latest_job:\n        return jsonify({\"status\": \"no_jobs\", \"message\": \"No processing jobs found.\"})\n\n    response = {\n        \"job_id\": latest_job.get(\"job_id\"),\n        \"status\": latest_job.get(\"status\"),\n        \"last_processed_at\": latest_job.get(\"last_processed_at\"),\n        \"summary\": latest_job.get(\"result\"),\n        \"error\": latest_job.get(\"error\"),\n    }\n    return jsonify(response)\n\nif __name__ == '__main__':\n    app.run(use_reloader=False, debug=True, host='0.0.0.0', port=8000, threaded=True)\n"
    }
}