{
    "entity_models": [
        {
            "entity_model_name": "book",
            "workflow_function": {
                "code_with_necessary_imports_and_constants": "from datetime import datetime\nfrom typing import Dict, Any\nimport logging\nimport asyncio\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# Constants and caches used in workflow\nbook_cache: Dict[str, Dict[str, Any]] = {}\nsearch_count: Dict[str, int] = {}\n\nOPEN_LIBRARY_WORKS_API = \"https://openlibrary.org{work_key}.json\"\n\nasync def fetch_work_description(work_key: str) -> str:\n    url = OPEN_LIBRARY_WORKS_API.format(work_key=work_key)\n    async with httpx.AsyncClient(timeout=10.0) as client:\n        try:\n            resp = await client.get(url)\n            resp.raise_for_status()\n            data = resp.json()\n            desc = data.get(\"description\")\n            if isinstance(desc, dict):\n                return desc.get(\"value\")\n            elif isinstance(desc, str):\n                return desc\n        except Exception as e:\n            logger.warning(f\"Failed to fetch description for {work_key}: {e}\")\n    return None\n\n# In-memory caches for search and recommendations\nbook_cache = {}\nsearch_count = {}\n",
                "content": "async def process_book(entity: Dict[str, Any]) -> None:\n    \"\"\"\n    This workflow enriches the book entity, updates cache and search counts.\n    It fetches detailed description asynchronously and adds timestamp.\n    \"\"\"\n\n    entity[\"processed_at\"] = datetime.utcnow().isoformat()\n\n    # Fetch and enrich description (async fetch)\n    work_key = f\"/works/{entity.get('book_id')}\"\n    description = await fetch_work_description(work_key)\n    if description:\n        entity[\"description\"] = description\n\n    # Example: You could also enrich genre here by fetching other sources or logic\n    # For demo, we leave genre empty or as is\n\n    # Update in-memory caches (allowed since these are not persistent entities)\n    book_cache[entity[\"book_id\"]] = entity\n    # Initialize search count if not present\n    if entity[\"book_id\"] not in search_count:\n        search_count[entity[\"book_id\"]] = 0",
                "name": "process_book"
            }
        }
    ],
    "file_without_workflow": {
        "code": "from dataclasses import dataclass\nfrom typing import Optional, Dict, Any, List\nimport asyncio\nimport logging\nfrom datetime import datetime\n\nimport httpx\nfrom quart import Quart, request, jsonify\nfrom quart_schema import QuartSchema, validate_request\n\nfrom app_init.app_init import BeanFactory\nfrom common.config.config import ENTITY_VERSION\n\nfactory = BeanFactory(config={'CHAT_REPOSITORY': 'cyoda'})\nentity_service = factory.get_services()['entity_service']\ncyoda_auth_service = factory.get_services()[\"cyoda_auth_service\"]\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\napp = Quart(__name__)\nQuartSchema(app)\n\n@dataclass\nclass Filters:\n    genre: Optional[str] = None\n    author: Optional[str] = None\n    publication_year: Optional[int] = None\n\n@dataclass\nclass SearchRequest:\n    query: str\n    filters: Filters = Filters()\n    page: int = 1\n    page_size: int = 20\n\n@dataclass\nclass RecommendationRequest:\n    limit: int = 10\n\nOPEN_LIBRARY_SEARCH_API = \"https://openlibrary.org/search.json\"\nOPEN_LIBRARY_COVER_URL = \"https://covers.openlibrary.org/b/id/{cover_id}-M.jpg\"\nOPEN_LIBRARY_WORKS_API = \"https://openlibrary.org{work_key}.json\"\n\n# In-memory caches for search and recommendations\nbook_cache: Dict[str, Dict[str, Any]] = {}\nuser_search_history: Dict[str, List[Dict[str, Any]]] = {}\nsearch_count: Dict[str, int] = {}\n\ndef get_cover_url_from_doc(doc: Dict[str, Any]) -> str:\n    cover_id = doc.get(\"cover_i\")\n    if cover_id:\n        return OPEN_LIBRARY_COVER_URL.format(cover_id=cover_id)\n    return \"\"\n\ndef normalize_book_doc(doc: Dict[str, Any]) -> Dict[str, Any]:\n    # minimal normalization, detailed enrichment will be in workflow\n    return {\n        \"book_id\": doc.get(\"key\", \"\").replace(\"/works/\", \"\"),\n        \"title\": doc.get(\"title\", \"\"),\n        \"author\": \", \".join(doc.get(\"author_name\", [])) if doc.get(\"author_name\") else \"\",\n        \"genre\": \"\",  # to be enriched later\n        \"publication_year\": doc.get(\"first_publish_year\"),\n        \"cover_image\": get_cover_url_from_doc(doc),\n        \"description\": None  # to be enriched later\n    }\n\nasync def fetch_work_description(work_key: str) -> Optional[str]:\n    # Fetch description from works API for enrichment\n    url = OPEN_LIBRARY_WORKS_API.format(work_key=work_key)\n    async with httpx.AsyncClient(timeout=10.0) as client:\n        try:\n            resp = await client.get(url)\n            resp.raise_for_status()\n            data = resp.json()\n            desc = data.get(\"description\")\n            if isinstance(desc, dict):\n                return desc.get(\"value\")\n            elif isinstance(desc, str):\n                return desc\n        except Exception as e:\n            logger.warning(f\"Failed to fetch description for {work_key}: {e}\")\n    return None\n\ndef filter_books(books: List[Dict[str, Any]], genre: Optional[str], author: Optional[str], publication_year: Optional[int]) -> List[Dict[str, Any]]:\n    filtered = []\n    for book in books:\n        if genre and book.get(\"genre\") and genre.lower() != book.get(\"genre\", \"\").lower():\n            continue\n        if author and author.lower() not in book.get(\"author\", \"\").lower():\n            continue\n        if publication_year and publication_year != book.get(\"publication_year\"):\n            continue\n        filtered.append(book)\n    return filtered\n\nasync def calculate_recommendations(user_id: str, limit: int) -> List[Dict[str, Any]]:\n    history = user_search_history.get(user_id, [])\n    if not history:\n        top_ids = sorted(search_count, key=lambda k: search_count[k], reverse=True)[:limit]\n        return [book_cache[i] for i in top_ids if i in book_cache]\n    authors = {f[\"filters\"][\"author\"].lower() for f in history if f[\"filters\"].get(\"author\")}\n    genres = {f[\"filters\"][\"genre\"].lower() for f in history if f[\"filters\"].get(\"genre\")}\n    candidates = []\n    for bid, book in book_cache.items():\n        a = book.get(\"author\", \"\").lower()\n        g = book.get(\"genre\", \"\").lower()\n        if (authors and any(x in a for x in authors)) or (genres and g in genres):\n            candidates.append(book)\n    sorted_cand = sorted(candidates, key=lambda b: search_count.get(b[\"book_id\"], 0), reverse=True)\n    return sorted_cand[:limit]\n\n@app.route(\"/api/books/search\", methods=[\"POST\"])\n@validate_request(SearchRequest)\nasync def search_books(data: SearchRequest):\n    if not data.query:\n        return jsonify({\"error\": \"Query parameter is required\"}), 400\n    user_id = request.headers.get(\"X-User-Id\", \"anonymous\")\n\n    # Use the in-memory cache for searching\n    # Filter cached books by query and filters\n    # Note: This is a simplified search, can be replaced with a real search backend\n\n    # Filter by query in title or author (case insensitive)\n    filtered_books = [\n        book for book in book_cache.values()\n        if data.query.lower() in book.get(\"title\", \"\").lower()\n        or data.query.lower() in book.get(\"author\", \"\").lower()\n    ]\n\n    # Apply filters\n    filtered_books = filter_books(filtered_books, data.filters.genre, data.filters.author, data.filters.publication_year)\n\n    # Pagination\n    start = (data.page - 1) * data.page_size\n    end = start + data.page_size\n    page_books = filtered_books[start:end]\n\n    # Track user search history and increase search counts\n    user_history = user_search_history.setdefault(user_id, [])\n    user_history.append({\"query\": data.query, \"filters\": vars(data.filters), \"timestamp\": datetime.utcnow().isoformat()})\n    for b in page_books:\n        search_count[b[\"book_id\"]] = search_count.get(b[\"book_id\"], 0) + 1\n\n    return jsonify({\n        \"results\": page_books,\n        \"total_results\": len(filtered_books),\n        \"page\": data.page,\n        \"page_size\": data.page_size\n    })\n\n@app.route(\"/api/books/<string:book_id>\", methods=[\"GET\"])\nasync def get_book_details(book_id):\n    try:\n        item = await entity_service.get_item(\n            token=cyoda_auth_service,\n            entity_model=\"book\",\n            entity_version=ENTITY_VERSION,\n            technical_id=book_id\n        )\n        if not item:\n            return jsonify({\"error\": \"Book not found\"}), 404\n        return jsonify(item)\n    except Exception as e:\n        logger.exception(e)\n        return jsonify({\"error\": \"Internal Server Error\"}), 500\n\n@app.route(\"/api/users/<string:user_id>/search-history\", methods=[\"GET\"])\nasync def get_search_history(user_id):\n    return jsonify({\"user_id\": user_id, \"search_history\": user_search_history.get(user_id, [])})\n\n@app.route(\"/api/users/<string:user_id>/recommendations\", methods=[\"POST\"])\n@validate_request(RecommendationRequest)\nasync def get_recommendations(user_id, data: RecommendationRequest):\n    recs = await calculate_recommendations(user_id, data.limit)\n    return jsonify({\"user_id\": user_id, \"recommendations\": recs})\n\n@app.route(\"/api/reports/weekly\", methods=[\"GET\"])\nasync def get_weekly_report():\n    most = sorted(search_count.items(), key=lambda x: x[1], reverse=True)[:10]\n    most_list = [{\"book_id\": bid, \"title\": book_cache[bid][\"title\"], \"search_count\": cnt} for bid, cnt in most if bid in book_cache]\n\n    genre_count: Dict[str, int] = {}\n    author_count: Dict[str, int] = {}\n    for hist in user_search_history.values():\n        for e in hist:\n            f = e[\"filters\"]\n            if f.get(\"genre\"):\n                genre_count[f[\"genre\"]] = genre_count.get(f[\"genre\"], 0) + 1\n            if f.get(\"author\"):\n                author_count[f[\"author\"]] = author_count.get(f[\"author\"], 0) + 1\n\n    top_genres = [g for g, _ in sorted(genre_count.items(), key=lambda x: x[1], reverse=True)[:5]]\n    top_authors = [a for a, _ in sorted(author_count.items(), key=lambda x: x[1], reverse=True)[:5]]\n    return jsonify({\n        \"report_date\": datetime.utcnow().isoformat(),\n        \"most_searched_books\": most_list,\n        \"user_preferences_summary\": {\n            \"top_genres\": top_genres,\n            \"top_authors\": top_authors\n        }\n    })\n\n# Workflow function applied on 'book' entity before persistence\nasync def process_book(entity: Dict[str, Any]) -> None:\n    \"\"\"\n    This workflow enriches the book entity, updates cache and search counts.\n    It fetches detailed description asynchronously and adds timestamp.\n    \"\"\"\n\n    entity[\"processed_at\"] = datetime.utcnow().isoformat()\n\n    # Fetch and enrich description (async fetch)\n    work_key = f\"/works/{entity.get('book_id')}\"\n    description = await fetch_work_description(work_key)\n    if description:\n        entity[\"description\"] = description\n\n    # Example: You could also enrich genre here by fetching other sources or logic\n    # For demo, we leave genre empty or as is\n\n    # Update in-memory caches (allowed since these are not persistent entities)\n    book_cache[entity[\"book_id\"]] = entity\n    # Initialize search count if not present\n    if entity[\"book_id\"] not in search_count:\n        search_count[entity[\"book_id\"]] = 0\n\n@app.route(\"/api/ingestion/daily\", methods=[\"POST\"])\nasync def trigger_daily_ingestion():\n    \"\"\"\n    Fetch raw book data from Open Library and add them to entity service with workflow enrichment.\n    \"\"\"\n\n    async def ingestion_task():\n        logger.info(\"Starting daily ingestion\")\n        try:\n            # Fetch raw data (no enrichment here)\n            async with httpx.AsyncClient(timeout=10.0) as client:\n                resp = await client.get(OPEN_LIBRARY_SEARCH_API, params={\"q\": \"programming\", \"page\": 1, \"limit\": 50})\n                resp.raise_for_status()\n                raw = resp.json()\n\n            docs = raw.get(\"docs\", [])\n            # Normalize minimally, detailed enrichment in workflow\n            books = [normalize_book_doc(d) for d in docs]\n\n            # Add entities without workflow enrichment\n            tasks = []\n            for book in books:\n                tasks.append(entity_service.add_item(\n                    token=cyoda_auth_service,\n                    entity_model=\"book\",\n                    entity_version=ENTITY_VERSION,\n                    entity=book\n                ))\n            # Await all add_item calls concurrently\n            await asyncio.gather(*tasks)\n\n            logger.info(f\"Daily ingestion updated {len(books)} books\")\n\n        except Exception as exc:\n            logger.exception(f\"Error during daily ingestion: {exc}\")\n\n    asyncio.create_task(ingestion_task())\n    return jsonify({\"status\": \"processing\", \"message\": \"Daily ingestion started\"}), 202\n\n\nif __name__ == \"__main__\":\n    import sys\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\", stream=sys.stdout)\n    app.run(use_reloader=False, debug=True, host=\"0.0.0.0\", port=8000, threaded=True)\n"
    }
}